LLMs from scratch


transformer - paying particular attention to certain part of the inputs

pytorch

raw text -> pretrained LLM (foundational model) -> Train -> Labelled dataset & Fine tuned LLM

transformer architecture - "attention is all you need"

encoder: input text -> vectors
decoder: vectors -> output text

GTP - generative pretrained transformers

BERT - bidirectional encoder representations from transformers 

common crawl - 2gb file of web crawl that can be used for training

Byte pair encoding - BPE - algorithm to covert text to numbers
python library tiktoken


